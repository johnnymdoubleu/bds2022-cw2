---
title: "Assignment 2"
author: "Johnny Lee, s1687781"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setlocale("LC_ALL", "English")
library(caret)
library(data.table)
library(glmnet)
library(MASS)
library(knitr)
library(kableExtra)
library(pROC)
```

## Problem 1 (27 points)

File wdbc2.csv (available from the accompanying zip folder on Learn) refers to a study of breast cancer where the outcome of interest is the type of the tumour (benign or malignant, recorded in column "diagnosis"). The study collected 30 imaging biomarkers on 569 patients.

### Problem 1.a (7 points)

Using package caret, create a data partition so that the training set contains 70% of the observations (set the random seed to 984065 beforehand). Fit both a ridge regression model and a lasso model which uses cross-validation on the training set to diagnose the type of tumour from the 30 biomarkers. Then use a plot to help identify the penalty parameter $\lambda$ that maximizes the AUC. Note: There is no need to use the prepare.glmnet() function from lab 4, using as.matrix() with the required columns is sufficient.

### Answer

```{r}
breast <- read.csv("data_assignment2/wdbc2.csv")
```

```{r}
set.seed(984065)

#Splitting into training and testing data set
breast$diagnosis <- ifelse(breast$diagnosis=="benign", 0, 1)
split.index <- createDataPartition(breast$diagnosis, 
                                   p = .7, list = TRUE)$Resample1
#train and test data sets
train.breast <- breast[split.index, ]
test.breast <- breast[-split.index, ]

#define explanatory and response variable matrix
biomarkers.x <- as.matrix(subset(train.breast, select = -c(id, diagnosis)))
biomarkers.y <- as.matrix(subset(train.breast, select = c(diagnosis)))
```

```{r}
set.seed(984065)
#Fitting ridge regression on training data
fit.lasso <- cv.glmnet(biomarkers.x, biomarkers.y , alpha = 1, family = "binomial", type.measure = "auc")
#Fitting lasso regression on training data
fit.ridge <- cv.glmnet(biomarkers.x, biomarkers.y , alpha = 0, family = "binomial", type.measure = "auc")

par(mfrow=c(1,2), mar=c(4,4,5,2))
plot(fit.lasso, main="Lasso")
plot(fit.ridge, main="Ridge")                  
```

```{r}
cat("The optimal value of lambda is:", fit.ridge$lambda.min)
cat("The optimal value of lambda is:", fit.lasso$lambda.min)
```

\newpage

### Problem 1.b (2 points)

Create a data table that for each value of 'lambda.min' and 'lambda.1se' for each model fitted in problem 1.a reports: \* the corresponding AUC, \* the corresponding model size. Use 3 significant digits for floating point values and comment on these results. Hint: The AUC values are stored in the field called 'cvm'.

### Answer

```{r}
#retrieving lambda min from lasso and ridge
lambdamin.lasso <- fit.lasso$lambda.min
lambdamin.ridge <- fit.ridge$lambda.min
#Position of lambda min
index_lambdamin.lasso <- which(lambdamin.lasso == fit.lasso$lambda)
index_lambdamin.ridge <- which(lambdamin.ridge == fit.ridge$lambda)
#retrieving lambda lse from lasso and ridge
lambda1se.lasso <- fit.lasso$lambda.1se
lambda1se.ridge <- fit.ridge$lambda.1se
#Position of lambda lse 
index_lambda1se.lasso <- which(lambda1se.lasso == fit.lasso$lambda)
index_lambda1se.ridge <- which(lambda1se.ridge == fit.ridge$lambda)
#AUC values of each lambda values
AUC.lambdamin.lasso <- signif(fit.lasso$cvm[index_lambdamin.lasso],3)
AUC.lambda1se.lasso <- signif(fit.lasso$cvm[index_lambda1se.lasso],3)
AUC.lambdamin.ridge <- signif(fit.ridge$cvm[index_lambdamin.ridge],3)
AUC.lambda1se.ridge <- signif(fit.ridge$cvm[index_lambda1se.ridge],3)
#Model size of each lambda values
ms.lasso.min <- fit.lasso$nzero[index_lambdamin.lasso]
ms.lasso.1se <- fit.lasso$nzero[index_lambda1se.lasso]
ms.ridge.min <- fit.ridge$nzero[index_lambdamin.lasso]
ms.ridge.1se <- fit.ridge$nzero[index_lambda1se.lasso]
table <-data.table(model = c("Lasso.min", "Lasso.1se", 
                              "Ridge.min", "Ridge.1se"),
                    Lambda = c(signif(lambdamin.lasso,3),
                               signif(lambda1se.lasso,3),
                               signif(lambdamin.ridge,3),
                               signif(lambda1se.ridge,3)), 
                    ModelSize = c(ms.lasso.min, ms.lasso.1se, 
                                  ms.ridge.min, ms.ridge.1se),
                    AUC = c(AUC.lambdamin.lasso, AUC.lambda1se.lasso,
                            AUC.lambdamin.ridge, AUC.lambda1se.ridge))
kable(table, caption = "Lambda values with its model size and AUC")
```

\newpage

### Problem 1.c (7 points)

Perform both backward (we'll later refer to this as model B) and forward (model S) stepwise selection on the same training set derived in problem 1.a. Report the variables selected and their standardized regression coefficients in decreasing order of the absolute value of their standardized regression coefficient. Discuss the results and how the different variables entering or leaving the model influenced the final result.

### Answer

```{r include=FALSE}
#Define full model and null model
full.model <- glm(diagnosis~., data = train.breast)
null.model <- glm(diagnosis~1, data = train.breast)
#perform 
model.B<- stepAIC(full.model, direction = "back")
model.S<- stepAIC(null.model, scope = list(upper = full.model), direction = "forward")
```

The forward model takes a null model(which is a model without any feature) and only considers the intercept as a starting point and then progress towards the full model (with 30 features) by adding features. It can be seen here that the final model (forward) considers more features than the backward model. The forward model considers 15 features instead of 14 (in the backwards model)

```{r}
#Computing coefficients of each model and tabulating it.
B.coef <-model.B$coefficients
B.order<- order(abs(B.coef), decreasing = TRUE)
S.coef <-model.S$coefficients
S.order<- order(abs(S.coef), decreasing = TRUE)
table <- list(B.coef[B.order], S.coef[S.order])
kable(table, col.names = "Coefficients", caption = "Coefficients of Model B and Model S") %>% 
  kable_styling(latex_options = "hold_position")
```

\newpage

### Problem 1.d (3 points)

Compare the goodness of fit of model B and model S in an appropriate way.

### Answer

```{r}
#Testing goodness of fit of model B and model S
cat("Model B deviance: ", model.B$deviance)
cat("Model S deviance: ", model.S$deviance)
pchisq(model.B$null.deviance - model.B$deviance, df = 15, lower.tail = FALSE)
pchisq(model.S$null.deviance - model.S$deviance, df = 15, lower.tail = FALSE)
```

\newpage

### Problem 1.e (2 points)

Compute the training AUC for model B and model S.

### Answer

```{r}
model.B.auc <- roc(train.breast$diagnosis, model.B$fitted.values, plot = TRUE, 
    xlim = c(0,1), col="red")
model.S.auc <- roc(train.breast$diagnosis, model.S$fitted.values, plot = TRUE, 
    add = TRUE,col = "blue")
legend("topright", legend = c("Model B", "Model S"),
         col = c("red", "blue"), lty = 1, cex = 0.8, bty = "n")
```

\newpage

### Problem 1.f (6 points)

Use the four models to predict the outcome for the observations in the test set (use the lambda at 1 standard error for the penalised models). Plot the ROC curves of these models (on the sameplot, using different colours) and report their test AUCs. Compare the training AUCs obtained in problems 1.b and 1.e with the test AUCs and discuss the fit of the different models.

### Answer

```{r}

#Lasso model 
lasso.pred <- predict(fit.lasso, newx = as.matrix(test.breast[,-c(1,2)]), s="lambda.1se")
#Ridge Regression model 
ridge.pred <- predict(fit.ridge, newx = as.matrix(test.breast[,-c(1,2)]), s="lambda.1se")
#Model B
B.pred <- predict(model.B, newdata = test.breast, type = "response")
#Model S
S.pred <- predict(model.S, newdata = test.breast, type = "response")
#Plotting ROC
lasso.auc <- roc(test.breast$diagnosis, lasso.pred, plot = TRUE, xlim = c(0,1), col = "red")$auc
ridge.auc <- roc(test.breast$diagnosis, ridge.pred, plot = TRUE, col = "blue", add = TRUE)$auc
B.auc <- roc(test.breast$diagnosis, B.pred, plot = TRUE, col = "green", add = TRUE)$auc
S.auc <- roc(test.breast$diagnosis, S.pred, plot = TRUE, col = "yellow", add = TRUE)$auc
legend("topright", legend = c("Lasso", "Ridge","Model B","Model S"),
         col = c("red", "blue", "green", "yellow"), lty = 1, cex = 0.8, bty = "n")
#Compare the AUCs 
train.auc <- c(AUC.lambda1se.lasso, AUC.lambda1se.ridge, model.B.auc$auc, model.S.auc$auc)
test.auc <- c(lasso.auc, ridge.auc, B.auc, S.auc)
models <- c("Lasso Regression", "Ridge Regression", "Model B", "Model S")
# We make a table and voila! 
table <- data.table(models, train.auc, test.auc)
kable(table, caption = "Training and Testing Acuraccy of each Model")
```

\newpage

## Problem 2 (40 points)

File GDM.raw.txt (available from the accompanying zip folder on Learn) contains 176 SNPs to be studied for association with incidence of gestational diabetes (a form of diabetes that is specific to pregnant women). SNP names are given in the form "rs1234_X" where "rs1234" is the official identifier (rsID), and "X" (one of A, C, G, T) is the reference allele.

### Problem 2.a (3 points)

Read file GDM.raw.txt into a data table named gdm.dt. Impute missing values in gdm.dt according to SNP-wise median allele count.

### Answer

```{r}
gdm.dt <- data.table(fread("data_assignment2/GDM.raw.txt"))
```

```{r}
#Performing Imputation using median
for (colnm in colnames(gdm.dt,-1)){
  gdm.dt[[colnm]][is.na(gdm.dt[[colnm]])] <- median(gdm.dt[[colnm]], na.rm = T)
}
```

### Problem 2.b (8 points)

Write function univ.glm.test \<- function(x, y, order = FALSE) where x is a data table of SNPs, y is a binary outcome vector, and order is a boolean. The function should fit a logistic regression model for each SNP in x, and return a data table containing SNP names, regression coefficients, odds ratios, standard errors and p-values. If order is set to TRUE, the output data table should be ordered by increasing p-value.

### Answer

```{r}
univ.glm.test <- function(x, y, order = FALSE){
  stopifnot(nrow(x) == length(y))
  output <- data.table("SNP"=character(), 
                       "coefficients"=numeric(), "odds.ratios"=numeric(), 
                       "std.error"=numeric(), "p.value"=numeric())
  for (i in 1:ncol(x)){
    regr <- glm(y ~ x[[i]], family = binomial(link = "logit"))
    summarised <- coef(summary(regr))
  ## remove the row corresponding to the intercept and the column containing
  ## the t-value, then convert to a dataframe
    output <- rbind(output, list(names(x)[i], summarised[2,1],
                                 exp(summarised[2,1]), summarised[2,2],
                                 summarised[2,4]))
  }
  ## assign better column names
  # colnames(output) <- c("SNP", "Coefficients", "odds ratios", "std.error", "p.value")
  if(order == TRUE){
    output <- output[order(p.value)]
  }
  return(output)
}
```

### Problem 2.c (5 points)

Using function univ.glm.test(), run an association study for all the SNPs in gdm.dt against having gestational diabetes (column "pheno"). For the SNP that is most strongly associated to increased risk of gestational diabetes and the one with most significant protective effect, report the summary statistics from the GWAS as well as the 95% and 99% confidence intervals on the odds ratio.

### Answer

```{r}
x <- gdm.dt[, 4:ncol(gdm.dt)]
y <- gdm.dt[[3]]
study <- univ.glm.test(x, y)
kable(head(study), caption = "Logistic Regression on Pheno vs  each SNP") %>%
  kable_styling(latex_options = "hold_position")
```
```{r}
index <- which(study$odds.ratio == max(study$odds.ratio))
kable(study[index, ], 
      caption=" most strongly associated to increased risk of gestational diabetes") %>% 
  kable_styling(full_width = F, position = "center", latex_options = "hold_position")
```
```{r}
strong.coef <- study[index, ]$coefficients
strong.se <- study[index,]$std.error

confidence.int.95 <- round(exp(strong.coef + 1.96 * strong.se*c(-1,1)), 3)
confidence.int.99 <- round(exp(strong.coef + 2.576 * strong.se*c(-1,1)),3)

newindex <- which(study$odds.ratio < 1)
best <- study[newindex,]
# Select the SNP with lowest p value
index3 <- which(best$p.value == min(best$p.value))
best.SNP <- best[index3]
kable(best.SNP, 
      caption= "most protective effect on gestational diabetes") %>% 
  kable_styling(full_width = F, position = "center", latex_options = "hold_position")
#We now find the confidence interval for odds ratio
best.coef <- best.SNP$coefficients
best.se <- best.SNP$std.error
best.ci.95 <- round(exp(best.coef +1.96*best.se *c(-1,1)),3)
best.ci.99 <- round(exp(best.coef +2.576*best.se *c(-1,1)),3)
#Output

cat(" SNP most strongly associated to increased risk of gestational     diabetes is", 
    "\n 95% Confidence Inteval = ", confidence.int.95, 
    "\n 99% Confidence Interval = ", confidence.int.99)
cat("\n SNPs with most protective effect on gestational diabetes is", 
    "\n 95% Confidence Inteval = ", best.ci.95, 
    "\n 99% Confidence Interval = ", best.ci.99)
```
We can see that SNP rs1423096_T has the highest odds ratio (1.91758) and hence is the most strongly associated to increased risk of gestational diabetes. In fact, this SNP increases the odds of having gestational diabetes by about 92!. The SNP with most significant protective effect is rs2237897_T and it reduced the risk of diabetes by about 35%.

### Problem 2.d (4points)

Merge your GWAS results with the table of gene names provided in file GDM.annot.txt (available from the accompanying zip folder on Learn). For SNPs that have p-value $< 10^{-4}$ (hit SNPs) report SNP name, effect allele, chromosome number and corresponding gene name. Separately, report for each 'hit SNP' the names of the genes that are within a 1Mb window from the SNP position on the chromosome. Note: That's genes that fall within +/- 1,000,000 positions using the 'pos' column in the dataset.

### Answer

```{r}
gene.names <- fread("data_assignment2/GDM.annot.txt")
# study
new.study <- study
new.study <- new.study %>%
  .[, full.snp := SNP] %>%
  .[, c("SNP", "effect.alle") := do.call(Map, c(f = c, strsplit(SNP, "_")))]

merge.dt <- merge(new.study, gene.names, by.x = "SNP", by.y="snp", all = TRUE)
hit.snps <- merge.dt[p.value<1e-4]
kable(hit.snps[,c("SNP", "effect.alle","chrom","gene")], 
      caption= "SNPs that have p-value $< 10^{-4}$") %>% 
  kable_styling(latex_options = "hold_position")
```

```{r}
hit.snp.window <- data.table()
for (i in hit.snps$SNP){
  idx <- which(hit.snps$SNP == i)
  window.val <- merge.dt[(merge.dt$pos>= hit.snps$pos[idx] - 1e6) &
                           (merge.dt$pos<= hit.snps$pos[idx] + 1e6)]
  hit.snp.window <- rbind(hit.snp.window, window.val)
}
# Display the genes that fall within this window
kable(data.table(hit.snp.window$gene), col.names = "gene", 
      caption = "Gene within $1$Mb Windo") %>%
  kable_styling(latex_options = "hold_position")
```

### Problem 2.e (8 points)

Build a weighted genetic risk score that includes all SNPs with p-value $< 10^{-4}$, a score with all SNPs with p-value $< 10^{-3}$, and a score that only includes SNPs on the FTO gene (hint: ensure that the ordering of SNPs is respected). Add the three scores as columns to the gdm.dt data table. Fit the three scores in separate logistic regression models to test their association with gestational diabetes, and for each report odds ratio, 95% confidence interval and p-value.

### Answer

```{r warning=FALSE}
#Defining each weighted genetic risk score
hit.snp.1 <- merge.dt[p.value<1e-4]
gdm.1 <- gdm.dt[, .SD, .SDcols = merge.dt[p.value <1e-4]$full.snp]
wgrs.1 <- as.matrix(gdm.1) %*% hit.snp.1$coefficients

hit.snp.2 <- merge.dt[p.value<1e-3]
gdm.2 <- gdm.dt[, .SD, .SDcols = merge.dt[p.value <1e-3]$full.snp]
wgrs.2 <- as.matrix(gdm.2) %*% hit.snp.2$coefficients

hit.snp.3 <- merge.dt[gene=="FTO"]
gdm.3 <- gdm.dt[, .SD, .SDcols = merge.dt[gene=="FTO"]$full.snp]
wgrs.3 <- as.matrix(gdm.3) %*% hit.snp.3$coefficients

#Adding 3 columns to gdm.dt
scores <- c(wgrs.1,wgrs.2, wgrs.3)
gdm.dt <- gdm.dt %>% .[, score.1:=wgrs.1] %>%
  .[, score.2:=wgrs.2] %>% .[, score.3:=wgrs.3]
```

```{r}
y <- gdm.dt[[3]]
x <- gdm.dt[,180:182]
wgrs.snp <- univ.glm.test(x, y)
wgrs.snp <- wgrs.snp %>% 
  .[,upper.conf.int:=round(exp(coefficients + 1.96 * std.error*-1), 3)] %>%
  .[,lower.conf.int:=round(exp(coefficients + 1.96 * std.error), 3)] %>%
  .[, !"coefficients"] %>% .[, !"std.error"]

kable(head(wgrs.snp), caption = "Logistic regression on Pheno vs SNP") %>%
  kable_styling(latex_options = "hold_position")
```

### Problem 2.f (4 points)

File GDM.test.txt (available from the accompanying zip folder on Learn) contains genotypes of another 40 pregnant women with and without gestational diabetes (assume that the reference allele is the same one that was specified in file GDM.raw.txt). Read the file into variable gdm.test. For the set of patients in gdm.test, compute the three genetic risk scores as defined in problem 2.e using the same set of SNPs and corresponding weights. Add the three scores as columns to gdm.test (hint: use the same columnnames as before).

### Answer

```{r}
gdm.test <- data.table(fread("data_assignment2/GDM.test.txt"))
```

### Problem 2.g (4 points)

Use the logistic regression models fitted in problem 2.e to predict the outcome of patients in gdm.test. Compute the test log-likelihood for the predicted probabilities from the three genetic risk score models.

### Answer

```{r}
# Enter code here.
```

### Problem 2.h (4points)

File GDM.study2.txt (available from the accompanying zip folder on Learn) contains the summary statistics from a different study on the same set of SNPs. Perform a meta-analysis with the results obtained in problem 2.c (hint: remember that the effect alleles should correspond) and produce a summary of the meta-analysis results for the set of SNPs with meta-analysis p-value $< 10^{-4}$ sorted by increasing p-value.

### Answer

```{r}
# Enter code here.
```

\newpage

## Problem 3 (33 points)

File nki.csv (available from the accompanying zip folder on Learn) contains data for 144 breast cancer patients. The dataset contains a binary outcome variable ("Event", indicating the insurgence of further complications after operation), covariates describing the tumour and the age of the patient, and gene expressions for 70 genes found to be prognostic of survival.

### Problem 3.a (6 points)

Compute the matrix of correlations between the gene expression variables, and display it so that a block structure is highlighted. Discuss what you observe. Write some code to identify the unique pairs of (distinct) variables that have correlation coefficient greater than 0.80 in absolute value and report their correlation coefficients.

### Answer

```{r}
nki <- read.csv("data_assignment2/nki.csv")
head(nki)
```

```{r}
cor(nki[,7:76])
```

### Problem 3.b (8 points)

Run PCA (only over the columns containing gene expressions), in order to derive a patient-wise summary of all gene expressions (dimensionality reduction). Decide which components to keep and justify your decision. Test if those principal components are associated with the outcome in unadjusted logistic regression models and in models adjusted for age, estrogen receptor and grade. Justify the difference in results between unadjusted and adjusted models.

### Answer

```{r}
# Enter code here.
```

### Problem 3.c (8 points)

Use plots to compare with the correlation structure observed in problem 2.a and to examine how well the dataset may explain your outcome. Discuss your findings and suggest any further steps if needed.

### Answer

```{r}
# Enter code here.
```

### Problem 3.d (11 points)

Based on the models we examined in the labs, fit an appropriate model with the aim to provide the most accurate prognosis you can for patients. Discuss and justify your decisions.

### Answer

```{r}
# Enter code here.
```
